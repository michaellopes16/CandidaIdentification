{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jiuxnZsa_GDK",
        "2SXH8jT__Qph"
      ],
      "authorship_tag": "ABX9TyNT5DM1wVz6zOixxzbrLK4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaellopes16/CandidaIdentification/blob/main/ExaplainableAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Libs**\n",
        "\n"
      ],
      "metadata": {
        "id": "AdaQttrzx8_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !jupyter nbconvert --to script ExplainableAPI.ipynb\n"
      ],
      "metadata": {
        "id": "76I9ZKEjPQ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install lime==0.1.1.37"
      ],
      "metadata": {
        "id": "3YGmvoFh7qXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f5e12b-ea85-40b9-91e8-4f6436cb5227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime==0.1.1.37\n",
            "  Downloading lime-0.1.1.37.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.0/276.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (1.13.1)\n",
            "Collecting progressbar (from lime==0.1.1.37)\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (1.3.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (0.23.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (2024.7.24)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (0.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime==0.1.1.37) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime==0.1.1.37) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime==0.1.1.37) (1.16.0)\n",
            "Building wheels for collected packages: lime, progressbar\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.1.1.37-py3-none-any.whl size=284260 sha256=8f27be0bffd8cf591d0e6e2cdf55fb78e2d9e62cf6e0462452698bac3551df9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/48/6c/2437c474145e879ae415fdce78be1ea7dc7e81f7f26900c667\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12066 sha256=242fe891e489b34d4611c8a75b8a09ddf045a79a278e68f07f938a0033b0698d\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/17/e5/765d1a3112ff3978f70223502f6047e06c43a24d7c5f8ff95b\n",
            "Successfully built lime progressbar\n",
            "Installing collected packages: progressbar, lime\n",
            "Successfully installed lime-0.1.1.37 progressbar-2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install shap==0.46.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ELXSZA_lED",
        "outputId": "6b4e79ce-ea39-471d-ecbd-44ed5fe680e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap==0.46.0\n",
            "  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (2.1.4)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (4.66.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (24.1)\n",
            "Collecting slicer==0.0.8 (from shap==0.46.0)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap==0.46.0) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap==0.46.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap==0.46.0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap==0.46.0) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap==0.46.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap==0.46.0) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap==0.46.0) (1.16.0)\n",
            "Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.46.0 slicer-0.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "aygJH5C7_KcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crie um classe em python chamada ExplainableAPI com o método load_data que recebe o caminho, o separador e retorna um x e um y\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "# from multipledispatch import dispatch\n",
        "\n",
        "import lime.lime_tabular # !pip install lime==0.1.1.37\n",
        "import shap #!pip install shap==0.46.0"
      ],
      "metadata": {
        "id": "I8Cfdiu97kAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GENERAL API**"
      ],
      "metadata": {
        "id": "pdStW9-5_MkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExplainableAPI:\n",
        "  QUERY_FUNGI = '''SELECT\n",
        "            f.name AS Fungi_Name,\n",
        "            v.VOC_Category,\n",
        "            COUNT(*) AS VOC_Count\n",
        "        FROM\n",
        "            FUNGI f\n",
        "        JOIN\n",
        "            FUNGI_VOC fv ON f.id = fv.FUNGI_ID\n",
        "        JOIN\n",
        "            VOCs v ON fv.VOC_ID = v.id_VOC\n",
        "        GROUP BY\n",
        "            f.name, v.VOC_Category\n",
        "        ORDER BY\n",
        "            f.name, v.VOC_Category,VOC_Count;\n",
        "        '''\n",
        "  QUERY_SENSOR = '''SELECT\n",
        "            f.name AS Sensor_Name,\n",
        "            v.VOC_Category,\n",
        "            COUNT(*) AS VOC_Count\n",
        "        FROM\n",
        "            SENSOR f\n",
        "        JOIN\n",
        "            SENSOR_VOC fv ON f.id = fv.SENSOR_ID\n",
        "        JOIN\n",
        "            VOCs v ON fv.VOC_ID = v.id_VOC\n",
        "        GROUP BY\n",
        "            f.name, v.VOC_Category\n",
        "        ORDER BY\n",
        "            f.name,VOC_Count, v.VOC_Category;\n",
        "        '''\n",
        "  def __init__(self, features_name):\n",
        "    self.features_name = features_name;\n",
        "\n",
        "  def load_data(self, path, sep):\n",
        "      data = pd.read_csv(path, delimiter=sep,header=None)\n",
        "      print()\n",
        "      y = data.iloc[:, data.shape[1]-1].values\n",
        "      x = data.iloc[:,0: data.shape[1]-1].copy().values\n",
        "      return x, y\n",
        "\n",
        "  def create_df_2(self, list_weights):\n",
        "      df = pd.DataFrame({\n",
        "      'Feature': range(len(list_weights)),\n",
        "      'Weight': list_weights\n",
        "      })\n",
        "      return df\n",
        "\n",
        "\n",
        "  def create_df(self, list_weights, features):\n",
        "      df = pd.DataFrame({\n",
        "      'Feature': features,\n",
        "      'Weight': list_weights\n",
        "      })\n",
        "      return df\n",
        "\n",
        "  def weight_by_feature(self, features_name, weight_list):\n",
        "      # Converte a lista de pesos em um array NumPy para facilitar a manipulação\n",
        "      weight_array = np.array(weight_list)\n",
        "\n",
        "      # Calcula o número de linhas que o DataFrame terá\n",
        "      num_rows = len(weight_list) // len(features_name)\n",
        "\n",
        "      # Redimensiona o array para ter 'num_rows' linhas e 'len(features_name)' colunas\n",
        "      reshaped_array = weight_array[:num_rows * len(features_name)].reshape(num_rows, len(features_name))\n",
        "\n",
        "      # Cria o DataFrame a partir do array redimensionado\n",
        "      df = pd.DataFrame(reshaped_array, columns=features_name)\n",
        "      return df\n",
        "  def get_final_result(self, wigths, feature):\n",
        "      df = self.create_df(wigths, feature)\n",
        "      self.ploat_heatmap(df)\n",
        "      self.ploat_bar(df)\n",
        "      df1 = self.weight_by_feature(self.features_name, df.Weight)\n",
        "      df1.max().plot(kind='bar')\n",
        "      df1.mean().plot(kind='bar')\n",
        "      return df1\n",
        "  def get_most_important_features(self, dataFrame, top_feature_number):\n",
        "\n",
        "      df = self.weight_by_feature(self.features_name, dataFrame.Weight)\n",
        "\n",
        "      newDf = df.mean().reset_index()\n",
        "      newDf.columns = ['Column', 'Mean']\n",
        "      newDf = newDf.sort_values(by='Mean', ascending=False)\n",
        "      top_features = newDf.head(top_feature_number)\n",
        "      return top_features\n",
        "\n",
        "  def concat_all_mothods(self, means_lime, means_shap, means_grad):\n",
        "      scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "      # Calculando as médias das colunas\n",
        "      means_lime['Mean'] = scaler.fit_transform(means_lime[['Mean']])\n",
        "      means_shap['Mean'] = scaler.fit_transform(means_shap[['Mean']])\n",
        "      means_grad['Mean'] = scaler.fit_transform(means_grad[['Mean']])\n",
        "\n",
        "      # # Renomeando as colunas para facilitar a concatenação\n",
        "      # means_lime.columns = ['Column', 'Mean']\n",
        "      # means_shap.columns = ['Column', 'Mean']\n",
        "      # means_grad.columns = ['Column', 'Mean']\n",
        "      print(means_lime)\n",
        "      # Adicionando uma coluna para identificar o dataframe\n",
        "      means_lime['DataFrame'] = 'LIME'\n",
        "      means_shap['DataFrame'] = 'SHAP'\n",
        "      means_grad['DataFrame'] = 'GRAD'\n",
        "\n",
        "      # Concatenando os dataframes\n",
        "      means_all = pd.concat([means_lime, means_shap, means_grad])\n",
        "      print(means_all)\n",
        "      # means_all['Mean'] = scaler.fit_transform(means_all[['Mean']])\n",
        "      self.plot_bar_chart_all_methods(means_all)\n",
        "      return means_all, means_lime, means_shap, means_grad\n",
        "\n",
        "  def get_top_features(self,means_lime,means_shap,means_grad,top_index ):\n",
        "      df_sorted_1 = means_lime.sort_values(by='Mean', ascending=False)\n",
        "      df_sorted_2 = means_shap.sort_values(by='Mean', ascending=False)\n",
        "      df_sorted_3 = means_grad.sort_values(by='Mean', ascending=False)\n",
        "\n",
        "      # Selecionar as três primeiras linhas\n",
        "      top_3_1 = df_sorted_1.head(top_index)\n",
        "      top_3_2 = df_sorted_2.head(top_index)\n",
        "      top_3_3 = df_sorted_3.head(top_index)\n",
        "\n",
        "      return top_3_1, top_3_2, top_3_3\n",
        "  def get_features_in_common(self, df1, df2, df3):\n",
        "      means_all, means_lime, means_shap, means_grad = self.concat_all_mothods(df1, df2, df3)\n",
        "      top_3_1, top_3_2, top_3_3 = self.get_top_features(means_lime, means_shap, means_grad, 3 )\n",
        "      # Adicionar uma coluna de peso a cada DataFrame. Para remover os pesos, comentar essas linhas e a de média ponderada com o [X]\n",
        "      # top_3_1['Peso'] = 0.25\n",
        "      # top_3_2['Peso'] = 0.35\n",
        "      # top_3_3['Peso'] = 0.40\n",
        "\n",
        "      df_concat = pd.concat([top_3_1, top_3_2, top_3_3])\n",
        "      # Contar a frequência de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_concat['Column'].value_counts()\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "      # Criar um novo DataFrame com as linhas que têm 'Coluna' repetida\n",
        "      df_final = df_concat[df_concat['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      # Calcular a média ponderada para cada valor na coluna 'Coluna' [X]\n",
        "      # df_final = df_final.groupby('Column').apply(lambda x: (x['Peso'] * x.drop(columns=['Peso'])).sum() / x['Peso'].sum())\n",
        "\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "      return df_final\n",
        "  def get_sensor_repeats(self, df_final):\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "      df_counts = pd.DataFrame({\n",
        "          'Sensors': coluna_counts.index,\n",
        "          'Repeats': coluna_counts.values\n",
        "      })\n",
        "      df_counts = df_counts.sort_values(by='Sensors').reset_index(drop=True)\n",
        "      return df_counts\n",
        "\n",
        "  def get_dict_by_query(self, query_sql, conn):\n",
        "      cursor = conn.cursor()\n",
        "      cursor.execute(query_sql)\n",
        "      conn.commit()\n",
        "      result = cursor.fetchall()\n",
        "      result_dict = {}\n",
        "      for data, category, count in result:\n",
        "          if data not in result_dict:\n",
        "              result_dict[data] = {}\n",
        "          result_dict[data][category] = count\n",
        "      return result_dict, result\n",
        "\n",
        "  # Função para verificar compatibilidade e retornar os 3 fungos mais compatíveis\n",
        "  def find_top_compatible_fungi(self, sensor,fungi_dict,sensor_dict ):\n",
        "      sensor_categories = sensor_dict.get(sensor, {})\n",
        "      compatibility = []\n",
        "\n",
        "      for fungus, categories in fungi_dict.items():\n",
        "          common_categories = set(sensor_categories.keys()).intersection(categories.keys())\n",
        "          if len(common_categories) >= 1:\n",
        "              common_details = {category: (sensor_categories[category], categories[category]) for category in common_categories}\n",
        "              compatibility.append((fungus, len(common_categories), common_details))\n",
        "\n",
        "      # Ordenar por número de categorias compatíveis (maior para menor)\n",
        "      compatibility.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      # Ordenar as categorias compatíveis por quantidade de repetições (maior para menor)\n",
        "      for i in range(len(compatibility)):\n",
        "          compatibility[i] = (compatibility[i][0], compatibility[i][1], dict(sorted(compatibility[i][2].items(), key=lambda item: item[1][1], reverse=True)))\n",
        "\n",
        "      # Retornar os 3 fungos mais compatíveis\n",
        "      return compatibility[:3]\n",
        "\n",
        "  def print_result(self, sensor,fungi_dict, sensor_dict, df_caounts ):\n",
        "      for sensor in df_caounts.Sensors:\n",
        "          print(sensor)\n",
        "          top_fungi = self.find_top_compatible_fungi(sensor,fungi_dict,sensor_dict)\n",
        "          print(f\"Os 3 fungos mais compatíveis com o sensor {sensor} são:\")\n",
        "          for fungus, count, details in top_fungi:\n",
        "              print(f\"{fungus} com {count} categorias compatíveis:\")\n",
        "              for category, (sensor_count, fungus_count) in details.items():\n",
        "                  print(f\"  - {category}: Sensor ({sensor_count} vezes), Fungo ({fungus_count} vezes)\")\n",
        "              print()\n",
        "\n",
        "  def plot_bar_chart_all_methods(self, dataFrame):\n",
        "      sns.set(style=\"whitegrid\")\n",
        "      # Criando a figura com maior DPI e tamanho\n",
        "      plt.figure(figsize=(12, 8), dpi=200)\n",
        "\n",
        "      # Criando o gráfico de barras\n",
        "      sns.barplot(x='Column', y='Mean', hue='DataFrame', data=dataFrame, palette='viridis')\n",
        "\n",
        "      # Adicionando labels e título\n",
        "      plt.xlabel('Sensors', fontweight='bold')\n",
        "      plt.ylabel('Average values', fontweight='bold')\n",
        "      plt.title('DataFrame Sensors Averages')\n",
        "\n",
        "      # Rotacionando os nomes das colunas no eixo X\n",
        "      plt.xticks(rotation=45)\n",
        "\n",
        "      # Exibindo o gráfico\n",
        "      plt.show()\n",
        "  # Adicionando uma coluna de índice para o eixo X\n",
        "  def plot_chart_line_df(self, dataFrame):\n",
        "      dataFrame['Index'] = dataFrame.index\n",
        "      # Transformando o DataFrame para o formato longo\n",
        "      df_long = pd.melt(dataFrame, id_vars=['Index'], var_name='Sensor', value_name='Valor')\n",
        "      # Plotando o gráfico de linha\n",
        "      plt.figure(figsize=(14, 8))\n",
        "      sns.lineplot(data=df_long, x='Index', y='Valor', hue='Sensor')\n",
        "      plt.xlabel('Index')\n",
        "      plt.ylabel('Value')\n",
        "      plt.title('Line chart sensors')\n",
        "      plt.legend(title='Sensors')\n",
        "      plt.show()\n",
        "\n",
        "  def ploat_heatmap(self, dataFrame):\n",
        "      fig = px.density_heatmap(dataFrame, x='Feature', y='Weight', nbinsx=20, nbinsy=20, color_continuous_scale='Viridis')\n",
        "      # Atualizar o layout para permitir zoom e melhor visualização\n",
        "      fig.update_layout(\n",
        "          title='Dynamic Heatmap of Feature Weightss',\n",
        "          xaxis=dict(title='Feature'),\n",
        "          yaxis=dict(title='Weight'),\n",
        "          autosize=True\n",
        "      )\n",
        "      # Mostrar o mapa de calor\n",
        "      fig.show()\n",
        "\n",
        "  def ploat_bar(self, dataFrame):\n",
        "      fig = px.bar(dataFrame, x='Feature', y='Weight', color='Weight', color_continuous_scale='Viridis')\n",
        "      # Atualizar o layout para permitir zoom e melhor visualização\n",
        "      fig.update_layout(\n",
        "          title='Feature Weights Bar Chart',\n",
        "          xaxis=dict(title='Feature'),\n",
        "          yaxis=dict(title='Weight'),\n",
        "          autosize=True\n",
        "      )\n",
        "      # Mostrar o gráfico de barras\n",
        "      fig.show()"
      ],
      "metadata": {
        "id": "HvN5VPYyyLmR"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LIME METHOD**"
      ],
      "metadata": {
        "id": "jiuxnZsa_GDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LIME_Method:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def run_LIME_Method(self, train_data, test_data,sample_index, model, class_names):\n",
        "    feature_names = np.array(range(train_data.shape[1]))\n",
        "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "        training_data=train_data,\n",
        "        feature_names=feature_names,\n",
        "        class_names=class_names,\n",
        "        discretize_continuous=False  # ou True, dependendo da natureza dos seus dados\n",
        "    )\n",
        "    num_features = train_data.shape[1]\n",
        "    print(num_features)\n",
        "    instance = test_data[sample_index]\n",
        "    explanation = explainer.explain_instance(\n",
        "        instance,\n",
        "        model.predict,\n",
        "        num_features= num_features\n",
        "    )\n",
        "    # Print the explanation\n",
        "    # for feature, weight in explanation.as_list():\n",
        "    #     print(f'{feature}: {weight}')\n",
        "    feature , weigths = zip(*sorted(explanation.as_list()))\n",
        "    weigths_array = np.array(weigths)\n",
        "    # weigths_matriz = weigths_array.reshape(1, -1)  # Redimensiona para 1 linha e 'n' colunas\n",
        "    return explanation, feature , weigths_array\n",
        "\n",
        "  def run_mulltiple_LIME_Method(self, train_data, test_data,sample_index, model, class_names,REPEATS):\n",
        "      df_results = []\n",
        "      for i in range(REPEATS):\n",
        "          print(f\"Cicle: {i+1} of {REPEATS}...\")\n",
        "          explanation, index, wiegths = self.run_LIME_Method(train_data=X_train, test_data=X_test, sample_index=SAMPLE_INDEX, model=model, class_names=class_names)\n",
        "          dataFrame = api.create_df(wiegths, index)\n",
        "          df = api.get_most_important_features(dataFrame, 3)\n",
        "          df_results.append(df)\n",
        "      return self.summarize_df(df_results)\n",
        "\n",
        "  def summarize_df(self, df_results):\n",
        "      #Colocar isso em um método\n",
        "      df_final = pd.concat(df_results, ignore_index=True)\n",
        "      # Contar a frequência de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "\n",
        "      # Criar um novo DataFrame com as linhas que têm 'Coluna' repetida\n",
        "      df_final = df_final[df_final['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      # Calcular a média das colunas repetidas\n",
        "      df_final = df_final.groupby('Column').mean().reset_index()\n",
        "\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "      return df_final\n",
        "\n",
        "  def plot_bar_chart(self, df_final):\n",
        "      # Configurar o estilo do Seaborn\n",
        "      sns.set(style=\"whitegrid\")\n",
        "\n",
        "      # Criar o gráfico de barras\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      barplot = sns.barplot(x='Column', y='Mean', hue='Column', data=df_final, palette='viridis')\n",
        "\n",
        "      # Adicionar títulos e rótulos\n",
        "      plt.xlabel('Sensores')\n",
        "      plt.ylabel('Média')\n",
        "      plt.title('Média dos Sensores para o método LIME')\n",
        "\n",
        "      # Mostrar o gráfico\n",
        "      plt.show()\n",
        "\n",
        "  def plot_class_proba(self, class_name, explanation):\n",
        "      # Seus valores de probabilidade\n",
        "      probabilidades = explanation.predict_proba\n",
        "      # Nomes das classes para o eixo x do gráfico\n",
        "      plt.figure(figsize=(10, 7))  # Ajuste o tamanho conforme necessário\n",
        "      # Criar o gráfico de barras\n",
        "      plt.bar(class_name, probabilidades)\n",
        "\n",
        "      # Adicionar título e rótulos aos eixos\n",
        "      plt.title('Probabilidades das Classes')\n",
        "      plt.xlabel('Classes')\n",
        "      plt.ylabel('Probabilidade')\n",
        "\n",
        "      # Mostrar o gráfico\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "-3nY0q2l5-10"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df1 = api.get_final_result(wieghts,index)"
      ],
      "metadata": {
        "id": "awhbAXd0m2w_"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SHAP METHOD**"
      ],
      "metadata": {
        "id": "2SXH8jT__Qph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SHAP_Method:\n",
        "  def __init__(self, features_name):\n",
        "    self.features_name = features_name\n",
        "  def run_SHAP_Method(self, train_data, test_data,sample_index, model, class_names, npermutations):\n",
        "    # explainer = shap.KernelExplainer(model.predict, train_data)\n",
        "    explainer = shap.PermutationExplainer(model.predict, train_data)\n",
        "    shap_values = explainer.shap_values(test_data[sample_index-1:sample_index], npermutations=npermutations)\n",
        "\n",
        "    predicted_classes, main_class_predicted = self.get_model_results(model, test_data,sample_index )\n",
        "\n",
        "    feature_weights = shap_values[:,:,main_class_predicted][0]  # Sum weights across classes #max_index\n",
        "    # culture_time_weigth = np.mean(feature_weights[0][train_data.shape[1]])\n",
        "    # mean_array = np.mean(feature_weights, axis=2)\n",
        "    # mean_array = mean_array[0][:-1]\n",
        "    # df2 = api.weight_by_feature(self.features_name, mean_array,culture_time_weigth)\n",
        "    return explainer,shap_values, feature_weights\n",
        "  def run_mulltiple_SHAP_Method(self, train_data, test_data,sample_index, model, class_names,REPEATS, npermutations):\n",
        "      df_results = []\n",
        "      for i in range(REPEATS):\n",
        "          print(f\"Cicle: {i+1} of {REPEATS}...\")\n",
        "          explanation, index, wieghts = self.run_SHAP_Method(train_data=X_train, test_data=X_test, sample_index=SAMPLE_INDEX, model=model, class_names=class_names, npermutations=npermutations)\n",
        "          dataFrame = api.create_df_2(wieghts)\n",
        "          df = api.get_most_important_features(dataFrame, 3)\n",
        "          df_results.append(df)\n",
        "      return self.summarize_df(df_results)\n",
        "\n",
        "  def get_model_results(self, model, test_data, sample_index):\n",
        "      predicted_classes = model.predict(test_data[sample_index-1:sample_index])\n",
        "      main_class_predicted = np.argmax(predicted_classes)\n",
        "      return predicted_classes, main_class_predicted\n",
        "\n",
        "  def summarize_df(self, df_results):\n",
        "      #Colocar isso em um método\n",
        "      df_final = pd.concat(df_results, ignore_index=True)\n",
        "      # Contar a frequência de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "\n",
        "      # Criar um novo DataFrame com as linhas que têm 'Coluna' repetida\n",
        "      df_final = df_final[df_final['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "      return df_final\n",
        "\n",
        "  def plot_bar_chart(self, df_final):\n",
        "      # Configurar o estilo do Seaborn\n",
        "      sns.set(style=\"whitegrid\")\n",
        "\n",
        "      # Criar o gráfico de barras\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      barplot = sns.barplot(x='Column', y='Mean', hue='Column', data=df_final, palette='viridis')\n",
        "\n",
        "      # Adicionar títulos e rótulos\n",
        "      plt.xlabel('Sensores')\n",
        "      plt.ylabel('Média')\n",
        "      plt.title('Média dos Sensores para o método LIME')\n",
        "\n",
        "      # Mostrar o gráfico\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "d5ImM-uC_UUc"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRAD-CAM**"
      ],
      "metadata": {
        "id": "am70VoG-Ptch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRAD_CAM_Method:\n",
        "  def __init__(self, features_name):\n",
        "    self.features_name = features_name\n",
        "\n",
        "  def run_GRAD_CAM_Method(self, test_data,sample_index, model, class_names, last_layer_name):\n",
        "      # Selecionar uma instância para visualização\n",
        "      instancia = test_data[sample_index]  # Por exemplo, a primeira instância do conjunto de dados\n",
        "\n",
        "      # Preparar a instância para o modelo (adicionar uma dimensão extra se necessário)\n",
        "      instancia = np.expand_dims(instancia, axis=0)\n",
        "\n",
        "      # Obter a saída do modelo para a instância selecionada\n",
        "      predicao = model.predict(instancia)\n",
        "\n",
        "      # Obter a classe prevista\n",
        "      classe_prevista = np.argmax(predicao[0])\n",
        "\n",
        "      # Obter o output do último layer convolucional\n",
        "      ultimo_conv_layer = model.get_layer(last_layer_name) #em alguns casos, isso pode mudar\n",
        "\n",
        "      # Criar um modelo para Grad-CAM\n",
        "      grad_model = tf.keras.models.Model(\n",
        "          [model.inputs],\n",
        "          [ultimo_conv_layer.output, model.output]\n",
        "      )\n",
        "\n",
        "      # Obter os gradientes da classe prevista em relação ao último layer convolucional\n",
        "      with tf.GradientTape() as tape:\n",
        "          conv_outputs, predictions = grad_model(instancia)\n",
        "          predictions = tf.convert_to_tensor(predictions)  # Converter para tensor\n",
        "\n",
        "          # Verificar o tamanho de predictions\n",
        "          num_classes = predictions.shape[-1]\n",
        "          if classe_prevista >= num_classes:\n",
        "              raise ValueError(f\"Classe prevista ({classe_prevista}) está fora dos limites (0 a {num_classes-1}).\")\n",
        "\n",
        "          # Garantir que predictions é um tensor de float32\n",
        "          predictions = tf.cast(predictions, tf.float32)\n",
        "          print(predictions[0][0][classe_prevista])\n",
        "          loss = predictions[0][0][classe_prevista] #predictions[:, classe_prevista]\n",
        "\n",
        "      # Gradientes em relação à saída do último layer convolucional\n",
        "      grads = tape.gradient(loss, conv_outputs)[0]\n",
        "\n",
        "      # Média ponderada dos canais da saída do layer convolucional\n",
        "      pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n",
        "\n",
        "      # Multiplicar cada canal na saída do feature map pelo \"importance\" desse canal\n",
        "      heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs[0]), axis=-1)\n",
        "      return heatmap, grad_model, classe_prevista\n",
        "      # Normalizar o heatmap\n",
        "      # heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "      # heatmap\n",
        "  def summarize_df(self, df_results):\n",
        "      # Concatenar os DataFrames\n",
        "      df_final = pd.concat(df_results, ignore_index=True)\n",
        "\n",
        "      # Contar a frequência de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "\n",
        "      # Criar um novo DataFrame com as linhas que têm 'Coluna' repetida\n",
        "      df_final = df_final[df_final['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      # Calcular a média das colunas repetidas\n",
        "      df_final = df_final.groupby('Column').mean().reset_index()\n",
        "\n",
        "      # Ordenar o DataFrame final pela coluna 'Coluna'\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "\n",
        "      return df_final\n",
        "\n",
        "  def run_mulltiple_GRAD_Method(self, test_data,sample_index, model, class_names,REPEATS, last_layer_name):\n",
        "      df_results = []\n",
        "      for i in range(REPEATS):\n",
        "          print(f\"Cicle: {i+1} of {REPEATS}...\")\n",
        "          heatmap, grad_model, classe_prevista = self.run_GRAD_CAM_Method(test_data=X_test, sample_index=SAMPLE_INDEX, model=model, class_names=class_names,last_layer_name=last_layer_name)\n",
        "          dataFrame = api.create_df_2(heatmap)\n",
        "          df = api.get_most_important_features(dataFrame, 3)\n",
        "          df_results.append(df)\n",
        "      return self.summarize_df(df_results)\n",
        "  def plot_heatmap(self, df_grad):\n",
        "      # Criar o gráfico de barras\n",
        "      fig = px.bar(df_grad, x='Feature', y='Weight', color='Weight', color_continuous_scale='Viridis')\n",
        "\n",
        "      # Atualizar o layout para permitir zoom e melhor visualização\n",
        "      fig.update_layout(\n",
        "          title='Gráfico de Barras dos Pesos das Características (Grad-CAM)',\n",
        "          xaxis=dict(title='Feature'),\n",
        "          yaxis=dict(title='Weight'),\n",
        "          autosize=True\n",
        "      )\n",
        "\n",
        "      # Mostrar o gráfico de barras\n",
        "      fig.show()\n",
        "\n",
        "  def plot_bar_chart(self, df_final):\n",
        "      # Configurar o estilo do Seaborn\n",
        "      sns.set(style=\"whitegrid\")\n",
        "\n",
        "      # Criar o gráfico de barras\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      barplot = sns.barplot(x='Column', y='Mean', hue='Column', data=df_final, palette='viridis')\n",
        "\n",
        "      # Adicionar títulos e rótulos\n",
        "      plt.xlabel('Sensores')\n",
        "      plt.ylabel('Média')\n",
        "      plt.title('Média dos Sensores para o método LIME')\n",
        "\n",
        "      # Mostrar o gráfico\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "zyP0mt07PwwZ"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataFrame = api.create_df_2(list_weights=heatmap)\n",
        "# df_GRAD = api.get_most_important_features(dataFrame, 3)"
      ],
      "metadata": {
        "id": "cmS7mzjjVKDY"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "sBr2zvsqWJWN"
      },
      "execution_count": 143,
      "outputs": []
    }
  ]
}